\documentclass[fontset=windows]{article}
\usepackage{anyfontsize}
\usepackage{ctex}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{floatrow}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{\textbf{决策树多文献调研报告}}
\author{PB21020659 曹宸瑞}

\begin{document}
\renewcommand{\figurename}{图}
\maketitle

% \tableofcontents
% \newpage

\ctexset{abstractname=摘要}
\begin{abstract}
      


\end{abstract}

\section{背景}

自20世纪50年代中期人工智能首次被公认为一门学科以来，机器学习一直是一个核心研究领域。在此之后，出现了ID3, C4.5, CART等决策树算法。

\section{决策树}

\subsection{定义}

从数据产生决策树的机器学习技术叫做决策树学习，通俗说就是决策树。一般的，一棵决策树如图\ref{fig:1}决策树模型所示，包含一个根结点、若干个内部结点和若干个叶结点。叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定测试序列。决策树是一个预测模型，代表的是对象属性与对象值之间的一种映射关系。数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测。

\begin{figure}[h]
\centering
\includegraphics[width=0.3\linewidth]{图片1.png}
\caption{\label{fig:1}决策树模型}
\end{figure}

\subsection{决策树生成}

决策树生成有以下步骤：

\begin{enumerate}
      \item 以资料母群体为根节点；
      \item 作单因子变异数分析等，找出变异量最大的变项作为分割准则；（决策树每个叶节点即为一连串法则的分类结果）
      \item 若判断结果的正确率或涵盖率未满足条件，则再依最大变异量条件长出分岔。
\end{enumerate}

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：

\begin{enumerate}
      \item 当前结点包含的样本全属于同一类别，无需划分；
      \item 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；
      \item 当前结点包含的样本集合为空，不能划分。
\end{enumerate}

在第2种情形下，我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；在第3种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。注意这两种情形的处理实质不同：情形2是在利用当前结点的后验分布，而情形3则是把父结点的样本分布作为当前结点的先验分布。

决策树学习的关键是如何选择最优划分属性。一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”(purity)越来越高。下文的三个算法中分别介绍了各自的划分准则。

\section{ID3}

ID3是由J. Ross Quinlan提出的一个决策树生成系统，可以处理噪声和不完整的信息。它是建立在奥卡姆剃刀（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。ID3算法的核心思想就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间。

\cite[Induction of Decision Trees]{ref1}一文关注机器学习以及用于构建简单的基于知识的系统的学习系统，在概述了机器学习特点并介绍了它的成员后，Quinlan详细描述了ID3系统，并介绍了对ID3的扩展，使其能够处理噪声和缺失值。

\subsection{准则——信息增益}

归纳任务的一种方法是生成所有可能的正确分类训练集的决策树，并选择其中最简单的一个。这种树的数量是有限的，但非常大，因此这种方法只适用于小型归纳任务。ID3算法则不一样，它有很多属性，训练集包含很多对象，但建立高效的的决策树却不需要太多的计算。

ID3的基本结构是迭代式的。随机选择训练集的一个子集，称为窗口，并从它形成一棵决策树，该树正确地对窗口中的所有对象进行分类。训练集中的所有其他对象都将使用该树进行分类。如果树对所有这些对象都给出了正确的答案，那么它对整个训练集也是正确的。否则，将向窗口中添加一组分类错误的对象，并继续该过程。这种迭代方法通常比直接从整个训练集生成决策树更快地找到正确的决策树。

ID3采用了基于信息的方法。假设C包含p个P类对象和n个N类对象，C的任何正确决策树将按照它们在C中的表示比例对对象进行分类，并返回一个类别。因此，决策树可以被视为P或N的来源，具有生成该消息所需的预期信息$$\text{I}\left(p,n\right)=-\frac{p}{p+n}\log_2{\left(\frac{p}{p+n}\right)}-\frac{n}{p+n}\log_2{\left(\frac{n}{p+n}\right)}$$这些信息在加权平均后获得一个期望信息$$\text{E}\left(A\right)=\sum_{i=1}^{v}{\frac{p_i+n_i}{p+n}I(p_i,n_i)}$$将信息增益定义为$$\text{Gain}\left(A\right)=I\left(p,n\right)-E(A)$$信息增益越高，划分策略越好。ID3检查所有候选属性并选择一个最大化增益，然后递归使用相同的过程。

ID3每次迭代的总计算需求与训练集的大小、属性的数量和决策树的非叶子节点的数量的乘积成正比。即使执行了多次迭代，这种关系也可以扩展到整个归纳过程。随着归纳任务维度的增加，没有观察到时间或空间的指数增长，因此该技术可以应用于大型任务。

这篇论文中所提及的预期信息I(p,n)在推广后就是\cite[机器学习]{ref5}书中的信息熵$$\text{Ent}\left(D\right)=-\sum_{k=1}{p_k\log_2{p_k}}$$$Ent(D)$的值越小，则D的纯度越高。一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。因此，我们可用信息增益来进行决策树的划分属性选择。

\subsection{噪声}

然而，真实世界的数据集是不准确的。对象的描述可能包含基于测量的属性，也可能包含基于主观判断的属性，这两种情况都会导致属性值的误差，训练集中的一些对象甚至可能被错误分类。对于有误差的训练集，决策树需要解释某些对象的特殊情况，因此会比正确的树复杂。这种属性值或类信息的非系统误差通常称为噪声。

如果要使树构建算法能够在受噪声影响的训练集上运行，则需要进行两个修改：(1)算法必须能够在属性不充分的情况下工作，因为噪声会导致即使是最全面的属性集也显得不充分；(2)算法必须能够判定进一步测试属性不会提高决策树的预测精度，应该避免增加决策树的复杂性，以适应单个噪声生成的特殊情况。

为了满足第一个需求，可以将类解释为概率，或者是选择数量较多的类。前者会最小化C中对象的误差平方和，而后者能最小化C中对象的绝对误差之和。

为了满足第二个需求，可以要求任何被测试属性的信息增益超过某个绝对或百分比阈值，但可能会导致性能下降；或者采用基于卡方检验的随机独立性方法，即借助一个有随机值的属性A所产生的比例，计算置信度以判断拒接或接受属性C。

\subsection{缺失值处理}

大部分资料表示ID3无法处理缺失值，但Quinlan在\cite[Induction of Decision Trees]{ref1}中有提及未知数据的处理方法。

现实生活中的数据集中可能有数据缺失或损坏，解决该问题的一种方法是尝试利用上下文提供的信息来填充未知值。与处理噪声相似，同样建立一个有随机值的属性A，通过待测试属性C中A的分布来猜测未知值。该方法可以直接选择最可能的值。或将对象划分为若干对象，每个对象都有一个可能的A值，并根据贝叶斯公式计算出的概率进行加权。论文中提供了相关的数学论证。

文中还提到了两种方法，并通过实验比较了它们的效果：
\begin{enumerate}
      \item 通过对C的子集C'建立决策树，根据决策树中A的分布来填充缺失值，但效果不甚理想；
      \item 将“未知”视为每个属性的新可能值，并以与其他值相同的方式处理它，但可能会导致信息增益出现异常。最终提出的行之有效的策略是，计算信息增益时按比例计算缺失值，分类时分入所有分支。这与下文提到的C4.5对缺失值处理的算法有类似之处。
\end{enumerate}

\section{C4.5}

在ID3中，没有剪枝策略，容易过拟合，且信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于1，且只能用于处理离散分布的特征。因此，Quinlan于1992年发表了\cite[C4. 5: programs for machine learning]{ref2}一文，提出了C4.5模型。

\subsection{准则——信息增益率}

C4.5算法与ID3算法相似，但对ID3算法进行了改进。信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5在生成的过程中，用信息增益率来选择特征。增益率定义为$$\text{Gainratio}\left(D,a\right)=\frac{Gain\left(D,a\right)}{IV\left(a\right)}$$其中$$\text{IV}\left(a\right)=-\sum_{v=1}^{V}{\frac{\left|D^V\right|}{\left|D\right|}\log_2{\frac{\left|D^V\right|}{\left|D\right|}}}$$被称为属性a的固有值。增益率准则对可取值数目较少的属性有所偏好，因此C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。


\subsection{剪枝}

由于过拟合的树在泛化能力的表现非常差，C4.5还引入了悲观剪枝策略进行后剪枝，这是一种后剪枝，如图\ref{fig:2}所示。悲观剪枝方法指的是，在已经生成的决策树上进行剪枝。C4.5用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。

与后剪枝对应的还有预剪枝\textsuperscript{图\ref{fig:6}}，即在节点划分前来确定是否继续增长。这种方法是在树的生长过程中设定一个指标，当达到该指标时就停止生长，但这样做容易产生“视界局限”，就是一旦停止分支，使得节点N成为叶节点，就断绝了其后继节点进行“好”的分支操作的任何可能性。不严格的说这会已停止的分支会误导学习算法，导致产生的树不纯度降差最大的地方过分靠近根节点。

一般情形下，后剪枝决策树通常比预剪枝决策树保留更多的分支，因而欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{图片6.png}
\caption{\label{fig:2}后剪枝决策树}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{图片5.png}
\caption{\label{fig:6}预剪枝决策树}
\end{figure}

\subsection{缺失值处理}

如果直接舍弃含有缺失值的元素，显然对决策树影响巨大，可能会导致最终结果有显著出入。\cite[机器学习]{ref5}一书提到C4.5算法可以通过在没有数据缺失的数据子集\textsuperscript{图\ref{fig:5}}中运行决策树算法，判断各个属性的优劣。

在特征值缺失的情况下，用没有缺失的样本子集所占比重来折算；选定一个划分特征，对于缺失该特征值的样本，将样本同时划分到所有子节点，但要调整样本的权重值。方法较为简单，但效果较好，对数据集图\ref{fig:5}的具体处理与最终生成结果不在此赘述，详见\cite[机器学习]{ref5}。

\begin{figure}[h]
\centering
\caption{\label{fig:5}有缺失值的数据集}
\includegraphics[width=0.6\linewidth]{表格1.png}
\end{figure}

\subsection{连续值处理}

由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对结点进行划分。此时，连续属性离散化技术可派上用场。最简单的策略是采用二分法对连续属性\textsuperscript{图\ref{fig:7}}进行处理。假设n个样本的连续特征有m个取值，C4.5将其排序并取相邻两样本值的平均数共m-1个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点。对数据集图\ref{fig:7}的具体处理与最终生成结果同样不在此赘述，详见\cite[机器学习]{ref5}。

值得一提的是，此方法就是老师上课讲的数据离散化，见图\ref{fig:3}。

\begin{figure}[h]
\centering
\caption{\label{fig:7}有连续值的数据集}
\includegraphics[width=0.6\linewidth]{表格2.png}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{图片3.png}
\caption{\label{fig:3}数据预处理-数据变换-离散化}
\end{figure}

\section{CART}

在ID3与C4.5之后，又提出了CART树，因为ID3和C4.5虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大。分类与回归树(classification and regression tree，CART)\cite[Breimanetal., 1984]{ref3}模型由Breiman等人在1984年提出，是应用广泛的决策树学习方法。分类树分析是当预计结果可能为离散类型（例如三个种类的花，输赢等）使用的概念。回归树分析是当局域结果可能为实数（例如房价，患者住院时间等）使用的概念。CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。CART算法的二分法可以简化决策树的规模，提高生成决策树的效率。相较于前两种算法，CART使用Gini系数作为变量的不纯度量，减少了大量的对数运算；采用“基于代价复杂度剪枝”方法进行剪枝；采用二叉树，运算速度快；采用代理测试来估计缺失值。

\subsection{准则———基尼指数}

CART决策树使用“基尼指数"(Gini index)来选择划分属性：$$Gini(D)=\sum_{k=1}^{|y|}\sum_{k'\neq k}p_kp_{k'}=1-\sum_{k=1}^{|y|}p_k^2$$直观来说，Gini(P)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据集D的纯度越高。基尼指数可以用来度量任何不均匀分布，是介于0~1之间的数，0是完全相等，1是完全不相等。对于二类分类问题，若样本点属于第1个类的概率是p，可以得到概率分布的基尼指数为$Gini(p)=2p(1-p)$。

类似于信息增益，有$$Gini(D,A)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}Dini(D^v)$$二分类则有$$Gini(D,A)=\frac{|D_1|}{|D|}Dini(D_1)+\frac{|D_2|}{|D|}Dini(D_2)$$其表示经$A=a$划分后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。于是在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性。同样的，基尼指数越低，划分策略越好。CART检查所有候选属性并选择一个最小化基尼指数，然后递归使用相同的过程。

由于$\ln(x)=-1+x+o(x)$，所以$$H(X)=-\sum_{k=1}^{K}p_k\ln{p_k}\approx \sum_{k=1}^{K}p_k(1-p_k)$$因此基尼指数可以理解为熵模型的一阶泰勒展开。\cite[统计与学习方法]{ref4}展示了图\ref{fig:4}显示二类分类问题中基尼指数Gini(p)(单位比特)之半H(p)/2和分类误差率的关系。横坐标表示概率p，纵坐标表示损失。可以看出基尼指数和熵之半的曲线很接近，都可以近似地代表分类误差率；但是相比对数运算，基尼指数计算时间明显较短。

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{图片4.png}
\caption{\label{fig:4}二类分类中基尼指数、熵之半和分类误差率的关系}
\end{figure}

\subsection{回归}

值得一提的是，CART不仅可以用于分类，还可以应用于回归。假设X与Y分别为输入和输出变量，并且Y是连续变量，对于训练数据集$D=\{(x_1,y_1),(x_2y_2),\dots,(x_N,y_N)\}$考虑如何生成回归树。对于连续值的处理，CART分类树采用基尼系数的大小来度量特征的各个划分点。在回归模型中，我们使用常见的和方差度量方式，对于任意划分特征A，对应的任意划分点s两边划分成的数据集$D_1$和$D_2$，求出使$D_1$和$D_2$各自集合的均方差最小，同时$D_1$和$D_2$的均方差之和最小所对应的特征和特征值划分点。

一棵回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为$$f(x)=\sum_{m=1}^{M}c_mI(x\in R_m)$$当输入空间的划分确定时，可以用平方误差$$\sum_{x_i\in R_m}(y_i-f(x_i))^2$$来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\hat{c_m}$是$R_m$上的所有输入实例$x_i$应的输出$y_i$的均值。

问题是怎样对输入空间进行划分。这里采用启发式的方法，选择第j个变量$x^{(j)}$和它取的值s，作为切分变量(splitting variable)和切分点(splitting point)，并定义两个区域：$$R_1(j,s)=\{x|x^{(j)}\leq s\}\ and\ R_2(j,s)=\{x|x^{(j)}>s\}$$然后寻找最优切分变量j和最优切分点，也就是求解$$min_{j,s}\left[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_i)^2+min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$对固定输入变量，可以找到最优切分点s。遍历所有输入变量，找到最优的切分变量j，构成一个对(j,s)。依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树(least squares regression tree)。具体实现请参考\cite[统计学习方法]{ref4}。

\subsection{剪枝}

CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小(模型变简单)，从而能够对未知数据有更准确的预测。CART枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\{T_0,T_1,\dots,T_n\}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。举个例子，对于原始的CART树$A_0$，先剪去一棵子树，生成子树$A_1$，然后再从$A_1$剪去一棵子树生成$A_2$，直到最后剪到只剩一个根结点的子树$A_n$。于是得到了$A_0-A_n$一共n+1棵子树。然后再用n+1棵子树预测独立的验证数据集，谁的误差最小就选谁。

简而言之，CART采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。

CART中定义了一个损失函数$$C_\alpha(T)=C(T)+\alpha|T|$$T为任意子树，C(T)为预测误差，|T|为子树T的叶子节点个数，$\alpha$是参数，C(T)衡量训练数据的拟合程度，|T|
衡量树的复杂度，$\alpha$权衡拟合程度与树的复杂度。令$\alpha$从0取到正无穷，对于每一个固定的$\alpha$，我们都可以找到使得$C_\alpha(T)$最小的最优子树$T(\alpha)$
。当$\alpha$很小的时候，$T_0$是最优子树；当$\alpha$最大时，单独的根节点是这样的最优子树。随着$\alpha$增大，我们可以得到一个这样的子树序列：$T_0,T_1,\dots,T_n$，这里的子树$T_{i+1}$生成是根据前一个子树$T_i$剪掉某一个内部节点生成的。

Breiman等人证明：将$\alpha$从小增大，$0=\alpha_0<\alpha_1<\dots<\alpha_n<\infty$，在每个区间$[\alpha_i,\alpha_{i+1})$中，子树$T_i$是这个区间里最优的。我们每次剪枝都是针对某个非叶节点，其他节点不变，所以我们只需要计算该节点剪枝前和剪枝后的损失函数即可。剪枝前以 t 节点为根节点的子树的损失函数是：$C_\alpha(T)=C(T_t)+\alpha|T|$，剪枝后的损失函数是$C_\alpha(T)=C(t)+\alpha$，通过 Breiman 证明我们知道一定存在一个$\alpha$使得$C_\alpha(T)=C_\alpha(t)$，使得这个值为：$$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$$$\alpha$的意义在于，$[\alpha_i,\alpha_{i+1})$中，子树$T_i$是这个区间里最优的。当$\alpha$大于这个值时，一定有$C_\alpha(T)>C_\alpha(t)$，也就是剪掉这个节点后都比不剪掉要更优。所以每个最优子树对应的是一个区间，在这个区间内都是最优的。

然后我们对$T_0$中的每个内部节点t都计算阈值：$$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$$它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$。$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树。

如此剪枝下去，直至得到根结点。在这一过程中，不断地增加$\alpha$的值，产生新的区间。


\subsection{缺失值处理}

对于如何在特征值缺失的情况下进行划分特征的选择，CART一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的20\%的记录是缺失的，那么这个特征就会减少20\%或者其他数值）。

对于缺失该特征值的样本该进行怎样处理，CART算法的机制是为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。

\subsection{连续值处理}

与C4.5的处理基本相同。

\section{总结与比较}

\begin{enumerate}
\item 以下是决策树的总结：
      \begin{enumerate}
      \item 决策树模型是表示基于特征对实例进行分类的树形结构。
      \item 决策树学习旨在构建一个与训练数据拟合很好，并且复杂度小的决策树。决策树学习算法包括3部分：特征选择、树的生成和树的剪枝。常用的算法有ID3、C4.5和CART。
      \item 特征选择的目的在于选取对训练数据能够分类的特征。特征选择的关键是其准则，常用的准则有信息增益（ID3）、信息增益比（C4.5）、基尼系数（CART）。
      \item 决策树的生成往往通过计算准则，从根结点开始，递归地产生决策树。这相当于用信息增益或其他准则不断地选取局部最优的特征，或将训练集分割为能够基本正确分类的子集。
      \item 由于生成的决策树存在过拟合问题，需要对它进行剪枝，以简化决策树。决策树的剪枝，往往从已生成的树上剪掉一些叶结点或叶结点以上的子树，并将其父结点或根结点作为新的叶结点，从而简化生成的决策树。
      \item 在信息增益、增益率、基尼指数之外，人们还设计了许多其他的准则用于决策树划分选择，然而有实验研究表明[Mingers, 1989b]，这些准则虽然对决策树的尺寸有较大影响，但对泛化性能的影响很有限。[Raileanu and Stoffel,2004]对信息增益和基尼指数进行的理论分析也显示出，它们仅在2\%的情况下会有所不同。剪枝方法和程度对决策树泛化性能的影响相当显著，有实验研究表明[Mingers, 1989a]，在数据带有噪声时通过剪枝甚至可将决策树的泛化性能提高25\%。
      \item 有一些决策树学习算法可进行“增量学习”(incremental learning)，即在接收到新样本后可对已学得的模型进行调整，而不用完全重新学习。主要机制是通过调整分支路径上的划分属性次序来对树进行部分重构，代表性算法
      有ID4 [Schlimmer and Fisher, 1986] > ID5R [Utgoff, 1989a] > ITI [Utgoff et al.,1997]等。增量学习可有效地降低每次接收到新样本后的训练时间开销，但多步增量学习后的模型会与基于全部数据训练而得的模型有较大差别。
      \end{enumerate}
\item 以下是三种算法的比较：
      \begin{enumerate}
      \item 划分标准的差异：ID3使用信息增益偏向特征值多的特征，C4.5使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART使用基尼指数克服C4.5需要求log的巨大计算量，偏向于特征值较多的特征。
      \item 使用场景的差异：ID3和C4.5都只能用于分类问题，CART可以用于分类和回归问题；ID3和C4.5是多叉树，速度较慢，CART是二叉树，计算速度很快；
      \item 样本数据的差异：ID3只能处理离散数据且缺失值敏感，C4.5和CART可以处理连续性数据且有多种方式处理缺失值。从样本量考虑的话，小样本建议C4.5、大样本建议CART。C4.5处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而CART本身是一种大样本的统计方法，小样本处理下泛化误差较大；
      \item 剪枝策略的差异：ID3没有剪枝策略，C4.5是通过悲观剪枝策略来修正树的准确性，而CART是通过代价复杂度剪枝。
      \end{enumerate}
\end{enumerate}

\ctexset{bibname=参考文献}
\begin{thebibliography}{100}  
\bibitem{ref1}Olshen R A, Quinlan J R. Induction of decision trees. Machine Learning, 1986, 1(1):81-106.
\bibitem{ref2}Olshen R A, Quinlan J R. C4. 5: programs for machine learning. Morgan Kaufmann, 1992.
\bibitem{ref3}Olshen R A, Breiman L, Friedman J Stone C. Classification and regression trees. Wadsworth, 1984.
\bibitem{ref4}李航. 统计学习方法[M]. 第2版. 北京: 清华大学出版社, 2019.
\bibitem{ref5}周志华. 机器学习[M]. 北京: 清华大学出版社, 2016.
\bibitem{ref6}【机器学习】决策树（上）——ID3、C4.5、CART, https://zhuanlan.zhihu.com/p/85731206
\end{thebibliography}

\end{document}
